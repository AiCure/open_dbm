"use strict";(self.webpackChunkopendbm_website=self.webpackChunkopendbm_website||[]).push([[3906],{5155:function(e){e.exports=JSON.parse('{"pluginId":"default","version":"2.0","label":"2.0","banner":"unmaintained","badge":true,"className":"docs-version-2.0","isLast":false,"docsSidebars":{"docs":[{"type":"category","label":"The Basics","collapsed":false,"collapsible":true,"items":[{"type":"link","label":"Introduction","href":"/open_dbm/docs/2.0/getting-started","docId":"getting-started"},{"type":"link","label":"Dependencies Installation","href":"/open_dbm/docs/2.0/dependencies-installation","docId":"dependencies-installation"},{"type":"link","label":"Installation for Beginner","href":"/open_dbm/docs/2.0/beginner-installation","docId":"beginner-installation"},{"type":"link","label":"Installation for Pro","href":"/open_dbm/docs/2.0/pro-installation","docId":"pro-installation"},{"type":"link","label":"More Resources","href":"/open_dbm/docs/2.0/more-resources","docId":"more-resources"}]},{"type":"category","label":"Using Docker OpenDBM","collapsed":false,"collapsible":true,"items":[{"type":"link","label":"Mac / Linux Usage","href":"/open_dbm/docs/2.0/opendbm-docker-usage","docId":"opendbm-docker-usage"},{"type":"link","label":"Windows Usage","href":"/open_dbm/docs/2.0/windows-usage","docId":"windows-usage"}]},{"type":"category","label":"OpenDBM Output","collapsed":false,"collapsible":true,"items":[{"type":"link","label":"Derived Variables","href":"/open_dbm/docs/2.0/opendbm-docker-output","docId":"opendbm-docker-output"},{"type":"link","label":"Raw Variables","href":"/open_dbm/docs/2.0/raw-variables","docId":"raw-variables"}]}],"variable":[{"type":"category","label":"Biomaker Variables","collapsed":false,"collapsible":true,"items":[{"type":"link","label":"Biomaker Variables","href":"/open_dbm/docs/2.0/biomaker-variables","docId":"biomaker-variables"},{"type":"category","label":"Facial Activity","collapsed":false,"collapsible":true,"items":[{"type":"link","label":"Facial Activity","href":"/open_dbm/docs/2.0/facial-activity","docId":"facial-activity"},{"type":"link","label":"Facial Landmark","href":"/open_dbm/docs/2.0/facial-landmark","docId":"facial-landmark"},{"type":"link","label":"Action units","href":"/open_dbm/docs/2.0/action-units","docId":"action-units"},{"type":"link","label":"Emotional Expressivity","href":"/open_dbm/docs/2.0/emotional-expressivity","docId":"emotional-expressivity"},{"type":"link","label":"Overall expressivity","href":"/open_dbm/docs/2.0/overall-expressivity","docId":"overall-expressivity"},{"type":"link","label":"Facial Asymmetry","href":"/open_dbm/docs/2.0/facial-asymmetry","docId":"facial-asymmetry"},{"type":"link","label":"Pain Expressivity","href":"/open_dbm/docs/2.0/pain-expressivity","docId":"pain-expressivity"}]},{"type":"category","label":"Verbal Acoustic","collapsed":false,"collapsible":true,"items":[{"type":"link","label":"Verbal Acoustic","href":"/open_dbm/docs/2.0/verbal-acoustic","docId":"verbal-acoustic"},{"type":"link","label":"Fundamental Frequency","href":"/open_dbm/docs/2.0/fundamental-frequency","docId":"fundamental-frequency"},{"type":"link","label":"Formant Frequencies","href":"/open_dbm/docs/2.0/formant-frequencies","docId":"formant-frequencies"},{"type":"link","label":"Audio Intensity","href":"/open_dbm/docs/2.0/audio-intensity","docId":"audio-intensity"},{"type":"link","label":"Harmonics-to-noise Ratio (HNR)","href":"/open_dbm/docs/2.0/harmonics-to-noise-ratio","docId":"harmonics-to-noise-ratio"},{"type":"link","label":"Glottal-to-noise Excitation Ratio (GNE)","href":"/open_dbm/docs/2.0/glottal-to-noise-excitation-ratio","docId":"glottal-to-noise-excitation-ratio"},{"type":"link","label":"Jitter","href":"/open_dbm/docs/2.0/jitter","docId":"jitter"},{"type":"link","label":"Shimmer","href":"/open_dbm/docs/2.0/shimmer","docId":"shimmer"},{"type":"link","label":"Pause Characteristics","href":"/open_dbm/docs/2.0/pause-characteristics","docId":"pause-characteristics"},{"type":"link","label":"Voice Prevalence","href":"/open_dbm/docs/2.0/voice-prevalence","docId":"voice-prevalence"}]},{"type":"category","label":"Speech","collapsed":false,"collapsible":true,"items":[{"type":"link","label":"Speech","href":"/open_dbm/docs/2.0/speech","docId":"speech"},{"type":"link","label":"Parts of Speech","href":"/open_dbm/docs/2.0/parts-of-speech","docId":"parts-of-speech"},{"type":"link","label":"Sentiment of Speech","href":"/open_dbm/docs/2.0/sentiment-of-speech","docId":"sentiment-of-speech"},{"type":"link","label":"Lexical Richness","href":"/open_dbm/docs/2.0/lexical-richness","docId":"lexical-richness"},{"type":"link","label":"Rate of Speech","href":"/open_dbm/docs/2.0/rate-of-speech","docId":"rate-of-speech"}]},{"type":"category","label":"Movement","collapsed":false,"collapsible":true,"items":[{"type":"link","label":"Head Movement","href":"/open_dbm/docs/2.0/head-movement","docId":"head-movement"},{"type":"link","label":"Eye Blink Behavior","href":"/open_dbm/docs/2.0/eye-blink-behavior","docId":"eye-blink-behavior"},{"type":"link","label":"Eye Gaze Directionality","href":"/open_dbm/docs/2.0/eye-gaze-directionality","docId":"eye-gaze-directionality"},{"type":"link","label":"Facial Tremor","href":"/open_dbm/docs/2.0/facial-tremor","docId":"facial-tremor"},{"type":"link","label":"Vocal Tremor","href":"/open_dbm/docs/2.0/vocal-tremor","docId":"vocal-tremor"}]}]}],"guidelines":[{"type":"category","label":"Data Guidelines","collapsed":false,"collapsible":true,"items":[{"type":"link","label":"Overview","href":"/open_dbm/docs/2.0/data-guidelines","docId":"data-guidelines"},{"type":"link","label":"Supported File Types","href":"/open_dbm/docs/2.0/supported-file-types","docId":"supported-file-types"},{"type":"link","label":"Video Guidelines","href":"/open_dbm/docs/2.0/video-guidelines","docId":"video-guidelines"},{"type":"link","label":"Audio Guidelines","href":"/open_dbm/docs/2.0/audio-guidelines","docId":"audio-guidelines"},{"type":"link","label":"Behavioral Considerations","href":"/open_dbm/docs/2.0/behavioral-considerations","docId":"behavioral-considerations"},{"type":"link","label":"Amount of Data Needed","href":"/open_dbm/docs/2.0/amount-data-needed","docId":"amount-data-needed"}]}]},"docs":{"action-units":{"id":"action-units","title":"Action units","description":"Action units (AUs) are individual facial musculature arrangements specified in the Facial Action Coding System (FACS), combinations of which can account for all possible facial expressions. OpenDBM outputs framewise values for AU presence and intensity for the following AUs:","sidebar":"variable"},"amount-data-needed":{"id":"amount-data-needed","title":"Amount of Data Needed","description":"This is one of the more common questions we get and I\u2019m sorry to say there really isn\u2019t one answer that applies here, as one can imagine. I can point to some of our work using OpenDBM and you can look at the sample sizes in there (which are fairly small). But here are some truths. More data is of course always better; no denying that. Video and audio data, though, is quite rich, and the length of behavior one would need to see effects (in our experience) is not hours, but minutes and sometimes just seconds. Importantly, this differs depending on the patient population and the behaviors during which the measures are being quantified. The only real advice we can provide in this matter is to search the literature and see what it says. In most cases, there is precedent to lean on. That can help with selecting the number of patients, time points, and length of video/audio needed to see effects.","sidebar":"guidelines"},"audio-guidelines":{"id":"audio-guidelines","title":"Audio Guidelines","description":"Similar to video, the assumption here is that it is the voice and speech of a patient that is being characterized. OpenDBM calculates acoustic measures from the sound wave that it is getting. So, if there is any sound in the audio file that is not the patient\u2019s voice, OpenDBM does not separate that out, and any subsequent measurements (think: the loudness of the sound, frequency of the waveform, and other acoustic features like the harmonics to noise ratio) will be from of all the sound in the audio file\u2013\u2013and not just the patient\u2019s voice. Similarly, if our objective is to characterize aspects of the speech, OpenDBM is transcribing all the speech that it can hear. So, if more than one person is speaking in the audio file, you\u2019re calculating variables from all of that speech\u2013\u2013not just the patient\u2019s. Below are some points to take into consideration.","sidebar":"guidelines"},"audio-intensity":{"id":"audio-intensity","title":"Audio Intensity","description":"Audio intensity is the loudness of a sound, measured in decibels (dB).","sidebar":"variable"},"beginner-installation":{"id":"beginner-installation","title":"Installation for Beginner","description":"Beginner Installation","sidebar":"docs"},"behavioral-considerations":{"id":"behavioral-considerations","title":"Behavioral Considerations","description":"Individual behavior is one of the more important aspects to consider when calculating digital biomarkers. OpenDBM is blind to the different behaviors the individual is participating in within the video or audio it is processing. For example, if in the same video, the individual is demonstrating spontaneous facial behavior (e.g. responding to an open-ended question) and then later in the video are asked to make a face (e.g. being asked to purse their lips, as some patients are during clinical assessments of facial tremor), OpenDBM is going to make its measurements across both behaviors. So, if the user is trying to measure spontaneous emotional expressivity, they really only want to do that in the former case; not the latter. Hence, when processing markers from data using OpenDBM, the user needs to split data by behavior.","sidebar":"guidelines"},"biomaker-variables":{"id":"biomaker-variables","title":"Biomaker Variables","description":"Biomaker Variables","sidebar":"variable"},"data-guidelines":{"id":"data-guidelines","title":"Overview","description":"OpenDBM Data Guidelines","sidebar":"guidelines"},"dependencies-installation":{"id":"dependencies-installation","title":"Dependencies Installation","description":"OpenDBM needs you to install some dependencies before do any pip install","sidebar":"docs"},"emotional-expressivity":{"id":"emotional-expressivity","title":"Emotional Expressivity","description":"Continuing to lean on FACS, action unit presence and intensity values are used to measure presence and intensity of emotional expressions, given that combinations of different action units refer to individual emotions as outlined in the table below:","sidebar":"variable"},"eye-blink-behavior":{"id":"eye-blink-behavior","title":"Eye Blink Behavior","description":"Eye blinks are measured by first calculating a variable called eye aspect ratio (EAR), which we get from here, and is basically just a quantification of how open the eye is. Over the course of a video, the EAR ends up being a vector whose troughs most likely signify individual eye blinks. The troughs are identified using a find peaks function and for each trough, the EAR value is outputted along with the other raw variables described below.","sidebar":"variable"},"eye-gaze-directionality":{"id":"eye-gaze-directionality","title":"Eye Gaze Directionality","description":"Eye gaze directionality is another output we get from OpenFace. The variables below allow for measurements of eye gaze behavior.","sidebar":"variable"},"facial-activity":{"id":"facial-activity","title":"Facial Activity","description":"Facial activity biomarkers relate to visually observable characteristics of the face i.e. movements and arrangements of facial musculature that can\u2013\u2013for example\u2013\u2013comprise emotional expressions. All facial features are acquired through use of the OpenFace software library.","sidebar":"variable"},"facial-asymmetry":{"id":"facial-asymmetry","title":"Facial Asymmetry","description":"Using facial landmark detection described in Section 5.1.1, an additional measurement that is made is that of facial asymmetry. Frame-wise and overall asymmetry in landmarks on the left vs. right side of the face is quantified and saved in the following raw and derived variables.","sidebar":"variable"},"facial-landmark":{"id":"facial-landmark","title":"Facial Landmark","description":"Facial landmarks refer to specific regions of the face, with x, y, and z coordinates for each facial landmark variable indicating where in the image frame that specific region of the face is located.","sidebar":"variable"},"facial-tremor":{"id":"facial-tremor","title":"Facial Tremor","description":"Measurements of facial tremor are acquired by looking at movements in individual facial landmarks as shown in Section 5.1.1. They are separated into measurements of tremor at individual landmarks of the face, which allows for specific calculation of tremor in particular areas of the face but can also be averaged to measure tremor in larger areas of the face or the entire face as a whole.","sidebar":"variable"},"formant-frequencies":{"id":"formant-frequencies","title":"Formant Frequencies","description":"Formant Frequencies (f1-4)","sidebar":"variable"},"fundamental-frequency":{"id":"fundamental-frequency","title":"Fundamental Frequency","description":"The fundamental frequency (f0) is the lowest frequency of a periodic waveform, measured in Hertz (Hz). It is the greatest common divisor of all the frequency components contained in a signal.","sidebar":"variable"},"getting-started":{"id":"getting-started","title":"Introduction","description":"This helpful guide lays out the prerequisites for learning OpenDBM, using these docs, and setting up your environment.","sidebar":"docs"},"glottal-to-noise-excitation-ratio":{"id":"glottal-to-noise-excitation-ratio","title":"Glottal-to-noise Excitation Ratio (GNE)","description":"Glottal-to-noise excitation ratio, as introduced by Michaelis and colleagues, is an indirect measure of breathiness, indicating whether a \u201cgiven voice signal originates from vibrations in the vocal folds or from turbulent noise generated in the vocal tract.\u201d","sidebar":"variable"},"harmonics-to-noise-ratio":{"id":"harmonics-to-noise-ratio","title":"Harmonics-to-noise Ratio (HNR)","description":"The harmonics-to-noise ratio (HNR), a common measurement of aspiration, quantifies the amount of additive noise in the voice signal.","sidebar":"variable"},"head-movement":{"id":"head-movement","title":"Head Movement","description":"Movement","sidebar":"variable"},"jitter":{"id":"jitter","title":"Jitter","description":"The jitter of an audio signal is the parameter of frequency variation from cycle to cycle, and is affected mainly by the lack of control over vocal cord vibration.","sidebar":"variable"},"lexical-richness":{"id":"lexical-richness","title":"Lexical Richness","description":"There are several terms for this measurement used across literature (sometimes also called diversity in vocabulary, etc.) and certainly more than one way to quantify it. We felt that an appropriate measure of richness of vocabulary would be the Moving Average Type Token Ratio (MATTR), reported in this paper by Convington and McFall. Simply put, it quantifies how many unique words are used in speech, which can be a proxy to some clinical measurements.","sidebar":"variable"},"more-resources":{"id":"more-resources","title":"More Resources","description":"There\u2019s always more to learn: how to use OpenDBM, OpenDBM Biomaker variables, Data Guidelines, and Resource","sidebar":"docs"},"opendbm-docker-output":{"id":"opendbm-docker-output","title":"Derived Variables","description":"OpenDBM Output","sidebar":"docs"},"opendbm-docker-usage":{"id":"opendbm-docker-usage","title":"Mac / Linux Usage","description":"Mac / Linux","sidebar":"docs"},"overall-expressivity":{"id":"overall-expressivity","title":"Overall expressivity","description":"Overall expressivity is a measure of facial expressivity regardless of emotional state. All action unit values are combined to determine the overall expressivity of the face, typically useful for measurement of blunted affect or disruption of facial expression as a consequence of motor retardation.","sidebar":"variable"},"pain-expressivity":{"id":"pain-expressivity","title":"Pain Expressivity","description":"Similar to emotion expressivity, pain expressivity is calculated using a combination of action units as defined in the Facial Action Coding System (FACS). The following action units have been found to be associated with pain according to reports in the scientific literature: 4 + 6 + 7 + 9 + 10 + 12 + 20 + 26.","sidebar":"variable"},"parts-of-speech":{"id":"parts-of-speech","title":"Parts of Speech","description":"Parts of speech simply describe aspects of the language used, such as usage of verbs, pronouns, adjectives, etc. along with other helpful measures such as the number of sentences spoken. These are acquired through NLTK.","sidebar":"variable"},"pause-characteristics":{"id":"pause-characteristics","title":"Pause Characteristics","description":"Fundamental frequency is usually zero when there is no voice detected. Using this understanding, frames where voice is or is not present can be determined and used to characterize pauses during speech and silence during the audio file. These metrics are quantified here.","sidebar":"variable"},"pro-installation":{"id":"pro-installation","title":"Installation for Pro","description":"Pro Installation","sidebar":"docs"},"rate-of-speech":{"id":"rate-of-speech","title":"Rate of Speech","description":"Rate of speech is simply a measure of words spoken per given unit of time. We measure this in words spoken per minute. However, for convenience, we are also outputting as a variable simply the length of the file that was analyzed and hence multiplying nlpwordsPerMinmean by nlptotalTimemean will give the total number of words spoken.","sidebar":"variable"},"raw-variables":{"id":"raw-variables","title":"Raw Variables","description":"Raw Variables","sidebar":"docs"},"sentiment-of-speech":{"id":"sentiment-of-speech","title":"Sentiment of Speech","description":"This refers to the emotional valence of the transcribed text based. This determination is based on pre-trained models found in the vaderSentiment library. Positive values of sentiment indicate positive emotional valence, while negative values indicate negative emotional valence.","sidebar":"variable"},"shimmer":{"id":"shimmer","title":"Shimmer","description":"The shimmer of an audio signal is the cycle-to-cycle amplitude variation of the sound wave.","sidebar":"variable"},"speech":{"id":"speech","title":"Speech","description":"The previous section focused on acoustic properties of speech. This section measures characteristics of language. First, any speech detected in the audio file is transcribed into text using DeepSpeech, an open-source speech transcription tool. From the transcribed text, a number of open source natural language processing tools are used to derive characteristics of speech.","sidebar":"variable"},"supported-file-types":{"id":"supported-file-types","title":"Supported File Types","description":"First and foremost\u2013\u2013OpenDBM only supports MP4 and MOV for video files and MP3 and WAV for audio files. Though there are several freely available online tools to convert file types quickly and easily, we advise that you be careful when using them given they do not guarantee data privacy and security. Instead, there are python packages available for local file conversion. Future versions of OpenDBM may support additional file types. For now, the user is responsible for doing this.","sidebar":"guidelines"},"tutorial":{"id":"tutorial","title":"Learn the Basics","description":"Tutorial template"},"verbal-acoustic":{"id":"verbal-acoustic","title":"Verbal Acoustic","description":"Verbal acoustics refer to measurements of the acoustic properties of an individual\u2019s voice.","sidebar":"variable"},"video-guidelines":{"id":"video-guidelines","title":"Video Guidelines","description":"Generally speaking\u2013\u2013and forgive me if this is too obvious\u2013\u2013OpenDBM is meant to calculate behavioral characteristics and subsequently derive digital biomarkers from video of an individual\u2019s head/face area and there is a basic expectation that the video will be of that individual (e.g. patient during a clinical interview) facing generally towards the direction of the camera lens. Some key points to consider regarding video that is processed are detailed below.","sidebar":"guidelines"},"vocal-tremor":{"id":"vocal-tremor","title":"Vocal Tremor","description":"Vocal tremor is measured according to the methods described in this paper by Marcus Bruckl and we highly suggest reading that work to help better understand the variables below.","sidebar":"variable"},"voice-prevalence":{"id":"voice-prevalence","title":"Voice Prevalence","description":"Using knowledge on which audio frames do and do not contain voice, the prevalence of voice as opposed to silence across the audio file is also calculated simply as a derived variable.","sidebar":"variable"},"windows-usage":{"id":"windows-usage","title":"Windows Usage","description":"Windows","sidebar":"docs"}}}')}}]);